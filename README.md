# neural-consensus# Neural Consensus

**Neural Fault Detection with Transfer Learning for Distributed Consensus**

A research implementation exploring whether neural networks can detect and classify node failures faster and more accurately than traditional timeout-based methods in distributed consensus systems.

## ğŸ¯ Research Question

> Can a neural network with transfer learning capabilities detect and classify node failures faster and more accurately than traditional timeout-based methods, while generalizing across different network deployments?

## ğŸ”¬ Key Innovations

1. **Predictive Failure Detection** â€” Detect failures *before* they happen using learned patterns
2. **Failure Classification** â€” Distinguish crash vs Byzantine vs partition vs slowdown
3. **Transfer Learning** â€” Train on one deployment, transfer to another with minimal fine-tuning

## ğŸ“ Project Structure
```
neural-consensus/
â”œâ”€â”€ simulation/              # Network simulation environment
â”‚   â”œâ”€â”€ clock.py            # Discrete event simulation clock
â”‚   â”œâ”€â”€ network.py          # Message passing with delays/loss/partitions
â”‚   â”œâ”€â”€ node.py             # Base node with failure injection
â”‚   â””â”€â”€ failures.py         # Failure injection strategies
â”‚
â”œâ”€â”€ protocols/raft/         # Raft consensus implementation
â”‚   â”œâ”€â”€ messages.py         # Raft message types (Vote, AppendEntries, etc.)
â”‚   â”œâ”€â”€ state.py            # Raft state management
â”‚   â””â”€â”€ node.py             # Complete Raft node
â”‚
â”œâ”€â”€ neural/                 # Neural network components
â”‚   â”œâ”€â”€ features.py         # Feature extraction from observations
â”‚   â”œâ”€â”€ encoder.py          # LSTM autoencoder
â”‚   â”œâ”€â”€ classifier.py       # Failure classification head
â”‚   â”œâ”€â”€ detector.py         # Neural failure detector
â”‚   â”œâ”€â”€ training.py         # Training loop
â”‚   â””â”€â”€ transfer.py         # Transfer learning utilities
â”‚
â”œâ”€â”€ data/                   # Data collection
â”‚   â”œâ”€â”€ collector.py        # Observation collector
â”‚   â””â”€â”€ labeler.py          # Auto-labeling
â”‚
â”œâ”€â”€ experiments/            # Experiment scripts
â”œâ”€â”€ configs/                # Configuration files
â”œâ”€â”€ models/                 # Saved models
â”œâ”€â”€ results/                # Experiment results
â””â”€â”€ tests/                  # Unit tests
```

## ğŸš€ Quick Start

### Installation
```bash
# Clone and enter directory
cd neural-consensus

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run tests
python test_all.py
```

### Train the Neural Detector
```bash
python train_detector.py
```

### Run Experiments
```bash
python run_experiments.py
```

## ğŸ§  Neural Architecture
```
Input: [20 observations Ã— 16 features]
              â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ LSTM Encoder  â”‚ (64 units, 2 layers)
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
      [32-dim latent space]
              â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
     â†“                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Decoder â”‚    â”‚  Classifier  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“                 â†“
Reconstruction    Failure Type
   Error          Prediction
(anomaly score)
```

### Failure Classes

| Class | Description |
|-------|-------------|
| 0 - Healthy | Normal operation |
| 1 - Pre-failure | About to fail (within 5s) |
| 2 - Crashed | Node has stopped |
| 3 - Byzantine | Malicious behavior |
| 4 - Partitioned | Network split |
| 5 - Slow | Degraded performance |

### Features (16 per observation)

- Latency: mean, std, trend, jitter
- Messages: rate, drop rate
- Heartbeats: regularity, missed count
- Response: rate, time
- Raft: term freshness, log/commit progress, leader status
- Composite: health score

## ğŸ“Š Experiments

### 1. Detection Speed
Compare time-to-detection between neural and timeout-based approaches.

### 2. False Positive Rate
Measure false alarms under various network conditions.

### 3. Classification Accuracy
Evaluate failure type classification with confusion matrix.

### 4. Transfer Learning
Test model transfer across different network deployments.

### 5. End-to-End Performance
Measure impact on consensus throughput, latency, and availability.

## ğŸ”§ Configuration

See `configs/default.yaml` for all options:
```yaml
neural_detector:
  window_size: 20
  encoder:
    hidden_size: 64
    latent_size: 32
  classifier:
    hidden_sizes: [64, 32]
    num_classes: 6
  training:
    epochs: 100
    learning_rate: 0.001
```

## ğŸ“ˆ Results

After training, results are saved to `results/`:
- `training_history.png` â€” Loss curves
- `confusion_matrix.png` â€” Classification performance
- `detection_latency.png` â€” Time to detect failures
- `transfer_performance.png` â€” Transfer learning results

## ğŸ”® Blockchain Applications

This research directly applies to:
- **Proof of Stake** validator monitoring (Ethereum, Solana)
- **BFT chains** (Cosmos/Tendermint, BNB Chain)
- **Layer 2** sequencer monitoring
- **Cross-chain bridges** validator security

## ğŸ“š References

1. Ongaro & Ousterhout. "In Search of an Understandable Consensus Algorithm" (Raft)
2. Castro & Liskov. "Practical Byzantine Fault Tolerance"
3. Kleppmann. "Designing Data-Intensive Applications"
4. Chandra & Toueg. "Unreliable Failure Detectors for Reliable Distributed Systems"

## ğŸ“„ License

MIT License

## ğŸ“– Citation
```bibtex
@article{neural-consensus-2025,
  title={Neural Fault Detection with Transfer Learning for Distributed Consensus},
  author={Prakhar},
  year={2025}
}
``` 